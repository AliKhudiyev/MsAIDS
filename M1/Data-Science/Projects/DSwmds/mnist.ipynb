{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data from keras\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], 'gray');\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Reshape the data for CNN\n",
    "# The images are gray level, so one channel is enough to represent the picture as it is\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 28, 28, 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# Transforming the labels (with one hot encoding)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the pixels\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# CNN Model\n",
    "\n",
    "CNNs are good at working with high dimensional data such as images. So, I am going to use it for this data set to classify the numbers between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 70)                322630    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                710       \n",
      "=================================================================\n",
      "Total params: 324,172\n",
      "Trainable params: 324,172\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv = Conv2D(32, kernel_size=(5,5), activation='relu', kernel_initializer='he_uniform', input_shape=(28,28,1))\n",
    "\n",
    "# Creating a sequential model\n",
    "model = Sequential(name='mnist_classifier')\n",
    "\n",
    "# Adding conv2d layer with the 5x5 kernel\n",
    "model.add(conv)\n",
    "# Adding max pooling layer to decrease the number parameters\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# Flattening the output of CNN\n",
    "model.add(Flatten())\n",
    "# A regular dense layer with 70 nodes\n",
    "model.add(Dense(70, activation='relu', kernel_initializer='he_uniform'))\n",
    "# Ouput layer with 10 nodes (since it is going to return one hot encoded version of the real label)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 0.1244 - accuracy: 0.9624\n",
      "Epoch 2/5\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 0.0460 - accuracy: 0.9855\n",
      "Epoch 3/5\n",
      "3750/3750 [==============================] - 13s 4ms/step - loss: 0.0286 - accuracy: 0.9908\n",
      "Epoch 4/5\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 0.0191 - accuracy: 0.9937\n",
      "Epoch 5/5\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 0.0131 - accuracy: 0.9956\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=5, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9876999855041504\n",
      "[[ 979    0    0    1    0    0    0    0    0    0]\n",
      " [   0 1135    0    0    0    0    0    0    0    0]\n",
      " [   3    1 1017    0    0    0    2    5    3    1]\n",
      " [   0    0    0  993    0   13    0    1    3    0]\n",
      " [   0    0    0    0  968    0    5    0    0    9]\n",
      " [   1    0    0    2    0  888    1    0    0    0]\n",
      " [   7    1    0    0    2    4  940    0    4    0]\n",
      " [   0    1    7    1    0    1    0 1015    1    2]\n",
      " [   4    0    1    0    0    0    0    1  966    2]\n",
      " [   2    3    0    3    5   11    0    7    2  976]]\n"
     ]
    }
   ],
   "source": [
    "_, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy:', acc)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = list(y_pred)\n",
    "y_pred = [list(y).index(max(y)) for y in y_pred]\n",
    "y_test_original = list(y_test)\n",
    "y_test_original = [list(y).index(1.0) for y in y_test_original]\n",
    "print(confusion_matrix(y_true=y_test_original, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SVM classifier\n",
    "\n",
    "Given all the pixels SVC should predict the category of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (10000, 784)\n",
      "Test data shape: (10000, 784)\n",
      "Accuracy: 0.917\n",
      "[[ 954    0    7    1    0    6    8    2    1    1]\n",
      " [   0 1121    1    2    0    2    3    1    5    0]\n",
      " [   8   11  930   13   11    4   13   11   29    2]\n",
      " [   3    2   28  914    2   20    2   10   21    8]\n",
      " [   2    1   12    0  927    0    6    6    2   26]\n",
      " [  12    6    6   60    7  754   15    1   26    5]\n",
      " [  12    3   12    1    9   12  906    0    2    1]\n",
      " [   3    8   24   15    9    0    0  943    5   21]\n",
      " [   7   18   10   31    9   32   11    6  840   10]\n",
      " [   8    8    1   15   46    6    1   37    6  881]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Loading the data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train, y_train = X_train[:10000], y_train[:10000]\n",
    "\n",
    "# Converting 28x28 2D array into 1D array of 784 elements(pixels)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1] * X_train.shape[2]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1] * X_test.shape[2]))\n",
    "\n",
    "# Changing the data type\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalizing the data\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print('Train data shape:', X_train.shape)\n",
    "print('Test data shape:', X_test.shape)\n",
    "\n",
    "# SVM classifier\n",
    "clf = SVC(C=1.0, kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy:', clf.score(X_test, y_test))\n",
    "print(confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# VPI model\n",
    "\n",
    "This model is slow for big data sets and especially with higher dimensions. So, I am going to add an extra preprocessing stage by subsampling the images first; that way the dimension becomes less problematic than it was before. This model works better when there is a correlation between the labels and the features; more features being similar implies the labels are similar as well and less features being similar implies the otherwise. The question is how to represent the image data so that this precondition of similarity is satisfied. The first simple idea is to represent the whole 2D image as a 1D array of values(pixels); if the images are so similar pixel-wisely then their labels should be the same as well, if not then they should probably represent different numbers and therefore, their labels are probably different. However, this idea can fail due to the several reasons:\n",
    "\n",
    "- What if there are two images with the same label but the pixel-wise comparison gives us high difference between the images(person A's notation of the number 7 may look different than person B's notation of the same number)?\n",
    "- What if there are two images with the same label that contain the numbers a bit shifted from the center and therefore, they are a bit away from each other when you overlap both images?\n",
    "- What if there are two images with the same label one of which contains a number but a bit rotated?\n",
    "- What if there are two images with the same label one of which contains a number which is the scaled version of the number represented on the other image?\n",
    "\n",
    "*Note:* In this data set, I think the first and second factors are the ones which may ruin the whole thing since the numbers are not rotated and scaled tremendously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Doing the same process with an additional subsampling\n",
    "(X_train2, y_train2), (X_test2, y_test2) = mnist.load_data()\n",
    "\n",
    "X_train3 = list()\n",
    "X_test3 = list()\n",
    "\n",
    "# Subsampling the train images\n",
    "for i in range(X_train2.shape[0]):\n",
    "    X_train3.append(cv2.resize(X_train2[i], (14, 14)))\n",
    "\n",
    "# Subsampling the test images\n",
    "for i in range(X_test2.shape[0]):\n",
    "    X_test3.append(cv2.resize(X_test2[i], (14, 14)))\n",
    "\n",
    "X_train3 = np.array(X_train3).astype('float32')\n",
    "X_test3 = np.array(X_test3).astype('float32')\n",
    "\n",
    "X_train3 /= 255\n",
    "X_test3 /= 255\n",
    "\n",
    "y_train2 = to_categorical(y_train2)\n",
    "y_test2 = to_categorical(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 14, 14) (10000, 14, 14)\n",
      "(60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train3.shape, X_test3.shape)\n",
    "print(y_train2.shape, y_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train3[0], 'gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 196) (10000, 196)\n",
      "(60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train3 = np.reshape(X_train3, (X_train3.shape[0], X_train3.shape[1]*X_train3.shape[2]))\n",
    "X_test3 = np.reshape(X_test3, (X_test3.shape[0], X_test3.shape[1]*X_test3.shape[2]))\n",
    "\n",
    "print(X_train3.shape, X_test3.shape)\n",
    "print(y_train2.shape, y_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92        85\n",
      "           1       0.67      1.00      0.80       126\n",
      "           2       0.98      0.52      0.68       116\n",
      "           3       0.81      0.80      0.81       107\n",
      "           4       0.83      0.74      0.78       110\n",
      "           5       0.90      0.69      0.78        87\n",
      "           6       0.84      0.92      0.88        87\n",
      "           7       0.82      0.81      0.82        99\n",
      "           8       0.86      0.63      0.73        89\n",
      "           9       0.63      0.90      0.74        94\n",
      "\n",
      "    accuracy                           0.79      1000\n",
      "   macro avg       0.82      0.79      0.79      1000\n",
      "weighted avg       0.82      0.79      0.79      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vpi\n",
    "import importlib\n",
    "importlib.reload(vpi)\n",
    "from vpi import *\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# With the traing set of 2000 images\n",
    "X_train4 = X_train3[:2000]\n",
    "y_train4 = y_train2[:2000]\n",
    "# With the test set of 1000 images\n",
    "X_test4 = X_test3[:1000]\n",
    "y_test4 = y_test2[:1000]\n",
    "\n",
    "y_pred = vpi(X_test4, X_train4, y_train4, n=70)\n",
    "y_test_ = [list(y).index(1) for y in y_test4]\n",
    "y_pred_ = [list(y).index(max(y)) for y in y_pred]\n",
    "\n",
    "print(classification_report(y_test_, y_pred_, target_names=[str(i) for i in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 79   0   0   0   1   1   3   0   0   1]\n",
      " [  0 126   0   0   0   0   0   0   0   0]\n",
      " [  1  31  60   5   2   1   3   7   6   0]\n",
      " [  0  10   0  86   0   2   2   2   2   3]\n",
      " [  0   3   0   0  81   0   2   0   0  24]\n",
      " [  2   4   0   6   3  60   3   2   1   6]\n",
      " [  2   0   0   0   3   2  80   0   0   0]\n",
      " [  0  10   0   1   1   0   0  80   0   7]\n",
      " [  3   4   1   7   3   1   2   3  56   9]\n",
      " [  0   1   0   1   4   0   0   3   0  85]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true=y_test_, y_pred=y_pred_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
