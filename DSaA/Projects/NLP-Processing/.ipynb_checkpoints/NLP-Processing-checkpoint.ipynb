{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility library\n",
    "\n",
    "This **CSV** class is to parse a csv file and dump them into json file.\n",
    "\n",
    "## Initialization\n",
    "\n",
    "The class gets initialized by passing a *filepath* of the csv file. It tries to parse the tokens not only by splitting for ','s but also matching '\"'s. If a line is ill-formed then it is ignored.\n",
    "\n",
    "## Serialization\n",
    "\n",
    "The member function **serialize()** is to create a python dictionary from the parsed context. It just iterates through the header of csv file and then for each header token it puts down all cells related to that header as its members. For example, <code>'{h1:{0:a, 1:b}, h2:{0:c, 1:d}}'</code>.\n",
    "\n",
    "## Jsonizing\n",
    "\n",
    "The member function **to_json(filepath)** is to create a json file which stores the given dictionary(in this case, obtained by a csv file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSV:\n",
    "    def __init__(self, filepath):\n",
    "        self.header = []\n",
    "        self.context = []\n",
    "        self.n_lines = 0\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            self.header = file.readline().split(',')\n",
    "            self.context = []\n",
    "\n",
    "            # Parsing the context\n",
    "            line = file.readline()\n",
    "            while len(line) > 0:\n",
    "                self.n_lines += 1 # line counter\n",
    "                beg = 0\n",
    "                tokens = [] # to hold the parsed tokens of each line\n",
    "                while beg < len(line):\n",
    "                    end = line.find(',', beg, len(line))\n",
    "                    if end == -1:\n",
    "                        end = len(line)\n",
    "                    \n",
    "                    n = line.count('\"', beg, end)\n",
    "                    while n > 0 and n%2 == 1:\n",
    "                        if line.count('\"', end, len(line)) == 0:\n",
    "                            break\n",
    "                        i = line.find(',', end + 1, len(line))\n",
    "                        n -= line.count('\"', end, i)\n",
    "                        end = i\n",
    "                    tokens.append(line[beg:end])\n",
    "                    beg = end + 1\n",
    "                \n",
    "                if len(tokens) == len(self.header): # Check if the line has been parsed correctly\n",
    "                    self.context.append(tokens)\n",
    "    #             else: # To see the ill-formed lines\n",
    "    #                 print('at',n_line,':',line)\n",
    "    #                 print(tokens)\n",
    "                line = file.readline()\n",
    "    \n",
    "    def serialize(self):\n",
    "        dictionary = {}\n",
    "        \n",
    "        for i, token in enumerate(self.header):\n",
    "            self.header[i]=self.header[i].replace('\\n', '')\n",
    "            dictionary[self.header[i]] = []\n",
    "        \n",
    "        for line in self.context:\n",
    "            for i, token in enumerate(line):\n",
    "                if len(token) > 0:\n",
    "                    token = token.replace('\\n', '')\n",
    "                    dictionary[self.header[i]].append(token)\n",
    "        \n",
    "        return dictionary\n",
    "    \n",
    "    def to_json(self, filepath):\n",
    "        dictionary = self.serialize()\n",
    "        string = '{'\n",
    "        \n",
    "        for i, key in enumerate(dictionary.keys()):\n",
    "            string += ('\\\"' + key + '\\\":{')\n",
    "            for j, value in enumerate(dictionary[key]):\n",
    "                string += '\\\"' + str(j) + '\\\":\\\"' + value + '\\\"'\n",
    "                if j < len(dictionary[key]) - 1:\n",
    "                    string += ','\n",
    "            string += '}'\n",
    "            if i < len(dictionary.keys()) - 1:\n",
    "                string += ','\n",
    "        string += '}'\n",
    "        \n",
    "        if len(filepath) > 0:\n",
    "            file = open(filepath, 'w', encoding='utf-8')\n",
    "            file.write(string)\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parsed lines: 12104 out of 12120\n",
      "Number of parsed lines: 5186  out of 5195\n"
     ]
    }
   ],
   "source": [
    "train = CSV('train.csv')\n",
    "print('Number of parsed lines:',len(train.context),'out of',train.n_lines)\n",
    "train.to_json('train.json')\n",
    "\n",
    "test = CSV('test.csv')\n",
    "print('Number of parsed lines:',len(test.context),' out of',test.n_lines)\n",
    "test.to_json('test.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
